#Streamlit permet de cr√©er des applications web interactives en Python, et est particuli√®rement adapt√© pour les projets de science des donn√©es et d'apprentissage automatique.
# Il est utilis√© ici pour cr√©er une interface utilisateur simple pour interagir avec le mod√®le de langage.
import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel, PeftConfig
# Chargement dU mod√®le de base GPT2
tokenizer = AutoTokenizer.from_pretrained("gpt2")
base_model = AutoModelForCausalLM.from_pretrained("gpt2")
base_model.eval() 
# Chargement de l'adaptateur LoRA
adapter_path = "C:/chemin/acces/modele/lora"  # Remplacer par le chemin vers l'adaptateur LoRA

# Chargement du mod√®le LoRA
config = PeftConfig.from_pretrained(adapter_path)
model = PeftModel.from_pretrained(base_model, adapter_path)
model.eval() 

# Configuration de la page web Streamlit
st.set_page_config(page_title="Chat IA", page_icon="ü§ñ")

# Titre principal affich√© sur la page
st.title("ü§ñ Chat avec ton mod√®le IA")

with st.form(key="chat_form"):
    # Zone de texte pour entrer une question ou une consigne pour l'IA
    prompt_input = st.text_area(
        "Pose une question √† l'IA :",  # label affich√©
        height=150,                    # hauteur de la zone de texte
        value="Tell me 3 things to do in Istanbul ?"  # texte pr√©-rempli
    )
    
    # Bouton pour envoyer la requ√™te
    submitted = st.form_submit_button("Envoyer")

if submitted:
    # Ajoute le format d'instruction du prompt
    full_prompt = f"### Instruction:\n{prompt_input}\n\n### Response:\n"
    
    # Tokenisation
    inputs = tokenizer(full_prompt, return_tensors="pt")
    ## G√©n√©rer une r√©ponse avec le mod√®le LoRA et GPT2, on prend les 50 meilleurs mots de la distribution avec une temp√©rature de 0.8
    # et une probabilit√© cumul√©e(top_p) d'au moins 0.95
    # et une longueur maximale de 100 tokens
    with torch.no_grad():
        outputs = model.generate(**inputs,
                                 do_sample=True,
                                 temperature=0.8,
                                 max_length=100,
                                 top_p=0.95,
                                 top_k=50,
                                 num_return_sequences=1)

        outputs2 = base_model.generate(**inputs,
                                       do_sample=True,
                                       temperature=0.8,
                                       max_length=100,
                                       top_p=0.95,
                                       top_k=50,
                                       num_return_sequences=1)

    # D√©codage de la r√©ponse du mod√®le LoRA
    response_lora = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # D√©codage de la r√©ponse du mod√®le de base
    response_base = tokenizer.decode(outputs2[0], skip_special_tokens=True)

    # Affichage stylis√© avec Markdown pour introduire la section LoRA
    st.markdown("### R√©ponse du mod√®le LoRA :")
    
    # Encadr√© vert pour mettre en valeur la r√©ponse principale (succ√®s)
    st.success(response_lora)

    # Affichage Markdown pour la comparaison
    st.markdown("### R√©ponse du mod√®le de base (GPT2) :")

    # Encadr√© bleu pour une information secondaire (r√©ponse du mod√®le de base)
    st.info(response_base)
